"""
ä½¿ç”¨vLLMå‘é‡æ¨¡å‹çš„RAGæµ‹è¯•
ä½¿ç”¨è¿œç¨‹192.168.1.232:8010çš„vLLM EmbeddingæœåŠ¡
"""
import sys
import asyncio
from pathlib import Path
from typing import List

# æ·»åŠ backendç›®å½•åˆ°Pythonè·¯å¾„
from pathlib import Path as PathType
backend_dir = PathType(__file__).parent.parent.parent.parent
sys.path.insert(0, str(backend_dir))

from langchain_core.documents import Document
from open_webui.test.services.vllm_embeddings import VLLMEmbeddings


class VLLMRAGTester:
    """ä½¿ç”¨vLLMçš„RAGæµ‹è¯•å™¨"""
    
    def __init__(self):
        self.embeddings = VLLMEmbeddings(api_url="http://192.168.1.232:8010")
        self.mineru_dir = Path("/home/zeroerr-ai72/openwebui-zeroerr/backend/data/uploads/knowledge/748b54f6-73b0-4efb-87c3-15c166556d6f/mineru")
        self.test_file = self.mineru_dir / "eRob_CANopen_and_EtherCATç”¨æˆ·æ‰‹å†Œv1.9" / "eRob_CANopen_and_EtherCATç”¨æˆ·æ‰‹å†Œv1.9.md"
        self.vector_store = {}  # ç®€å•çš„å†…å­˜å‘é‡å­˜å‚¨
    
    def load_markdown_file(self) -> Path:
        """åŠ è½½æµ‹è¯•markdownæ–‡ä»¶"""
        if not self.test_file.exists():
            print(f"âš ï¸  æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {self.test_file}")
            return None
        
        print(f"\nğŸ“‚ åŠ è½½æµ‹è¯•æ–‡ä»¶: {self.test_file.name}")
        print(f"   æ–‡ä»¶å¤§å°: {self.test_file.stat().st_size / 1024:.2f} KB")
        return self.test_file
    
    def chunk_markdown(self, content: str) -> List[Document]:
        """åˆ†æ®µmarkdownæ–‡ä»¶"""
        from langchain.text_splitter import MarkdownHeaderTextSplitter
        
        headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"),
            ("###", "Header 3"),
            ("####", "Header 4"),
        ]
        
        markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on,
            strip_headers=False
        )
        
        chunks = markdown_splitter.split_text(content)
        print(f"âœ… Markdownåˆ†æ®µå®Œæˆ: {len(chunks)} ä¸ª")
        return chunks
    
    def load_and_chunk_file(self, file_path: Path) -> List[Document]:
        """åŠ è½½å¹¶åˆ†æ®µæ–‡ä»¶"""
        print(f"\nğŸ“ å¼€å§‹åŠ è½½å’Œåˆ†æ®µ...")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            chunks = self.chunk_markdown(content)
            
            # æ·»åŠ æ–‡ä»¶æ¥æºä¿¡æ¯
            for chunk in chunks:
                if chunk.metadata is None:
                    chunk.metadata = {}
                chunk.metadata['source'] = file_path.name
            
            return chunks
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return []
    
    async def vectorize_and_store(self, chunks: List[Document]):
        """å‘é‡åŒ–å¹¶å­˜å‚¨"""
        if len(chunks) == 0:
            print("âš ï¸  æ²¡æœ‰æ–‡æ¡£å¯ä»¥å‘é‡åŒ–")
            return
        
        print(f"\nğŸ”¢ å‘é‡åŒ– {len(chunks)} ä¸ªæ–‡æ¡£...")
        
        # æå–æ–‡æœ¬å†…å®¹
        texts = [chunk.page_content for chunk in chunks]
        
        # å‘é‡åŒ–ï¼ˆä½¿ç”¨vLLM APIï¼‰
        try:
            vectors = self.embeddings.embed_documents(texts)
            print(f"âœ… å‘é‡åŒ–å®Œæˆ: {len(vectors)} ä¸ªå‘é‡")
            
            # å­˜å‚¨åˆ°å†…å­˜
            for i, (chunk, vector) in enumerate(zip(chunks, vectors)):
                self.vector_store[i] = {
                    'doc': chunk,
                    'vector': vector
                }
            
            print(f"âœ… å·²å­˜å‚¨ {len(self.vector_store)} ä¸ªå‘é‡")
        except Exception as e:
            print(f"âŒ å‘é‡åŒ–å¤±è´¥: {e}")
    
    async def vector_search(self, query: str, top_k: int = 5):
        """å‘é‡æ£€ç´¢"""
        if len(self.vector_store) == 0:
            print("âš ï¸  å‘é‡å­˜å‚¨ä¸ºç©º")
            return []
        
        print(f"\nğŸ” å‘é‡æ£€ç´¢: '{query}'")
        
        try:
            # å‘é‡åŒ–æŸ¥è¯¢
            query_vector = self.embeddings.embed_query(query)
            print(f"âœ… æŸ¥è¯¢å‘é‡ç»´åº¦: {len(query_vector)}")
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            import numpy as np
            query_array = np.array(query_vector)
            
            results = []
            for idx, item in self.vector_store.items():
                doc_vector = np.array(item['vector'])
                similarity = np.dot(query_array, doc_vector) / (
                    np.linalg.norm(query_array) * np.linalg.norm(doc_vector)
                )
                results.append((item['doc'], similarity))
            
            # æ’åº
            results.sort(key=lambda x: x[1], reverse=True)
            
            print(f"âœ… æ£€ç´¢å®Œæˆ: æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            return results[:top_k]
            
        except Exception as e:
            print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    async def test_retrieval(self):
        """æµ‹è¯•æ£€ç´¢"""
        print(f"\n{'='*70}")
        print("ğŸ” æµ‹è¯•æ£€ç´¢")
        print(f"{'='*70}")
        
        test_queries = [
            "CANopenæŠ¥æ–‡",
            "é‡å¤å®šä½ç²¾åº¦",
            "å…³èŠ‚å‹å·æŸ¥è¯¢",
        ]
        
        for query in test_queries:
            print(f"\n{'='*70}")
            print(f"æŸ¥è¯¢: {query}")
            print(f"{'='*70}")
            
            results = await self.vector_search(query, top_k=5)
            
            for i, (doc, score) in enumerate(results, 1):
                print(f"\n{i}. [ç›¸ä¼¼åº¦: {score:.4f}]")
                print(f"   ğŸ“„ {doc.page_content[:150]}...")
                print(f"   ğŸ“ æ¥æº: {doc.metadata.get('source', 'N/A')}")
    
    async def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„RAGç®¡é“"""
        print("="*70)
        print("ğŸš€ ä½¿ç”¨vLLMçš„RAGæµ‹è¯•")
        print("="*70)
        
        # 1. åŠ è½½æ–‡ä»¶
        md_file = self.load_markdown_file()
        if md_file is None:
            return
        
        # 2. åˆ†æ®µ
        chunks = self.load_and_chunk_file(md_file)
        if len(chunks) == 0:
            return
        
        # 3. å‘é‡åŒ–
        await self.vectorize_and_store(chunks)
        
        # 4. æµ‹è¯•æ£€ç´¢
        await self.test_retrieval()
        
        print("\n" + "="*70)
        print("âœ… æµ‹è¯•å®Œæˆ")
        print("="*70)


async def main():
    tester = VLLMRAGTester()
    await tester.run_full_pipeline()


if __name__ == "__main__":
    asyncio.run(main())
