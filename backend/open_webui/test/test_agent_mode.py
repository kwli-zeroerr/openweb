"""
æµ‹è¯• Agent æ¨¡å¼ä½¿ç”¨ LangGraph ç›´æ¥è°ƒç”¨å·¥å…·å‡½æ•°

å›ºå®šé—®é¢˜ï¼šæŸ¥è¯¢é›¶å·®äº‘æ§å…³èŠ‚é‡å¤å®šä½ç²¾åº¦ï¼Œå¹¶é¢„æµ‹äº§å“äº¤ä»˜æ—¶é—´

ä½¿ç”¨æ–¹æ³•:
    cd backend && python -m open_webui.test.test_agent_mode
"""
import asyncio
import sys
import logging
import json
import requests
from pathlib import Path
from typing import Any, Dict, Optional
from unittest.mock import MagicMock

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
backend_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(backend_dir))

from fastapi import Request
from open_webui.agent.graph import execute_workflow
from open_webui.agent.nodes import ToolNode
from open_webui.models.users import UserModel, UserSettings

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å›ºå®šæµ‹è¯•é—®é¢˜
FIXED_QUESTION = "æˆ‘æƒ³çŸ¥é“é›¶å·®äº‘æ§å…³èŠ‚é‡å¤å®šä½ç²¾åº¦æ˜¯å¤šå°‘ï¼Ÿè¯·è°ƒç”¨ç›¸å…³å·¥å…·æŸ¥è¯¢å¹¶ç»™å‡ºè¯¦ç»†ç­”æ¡ˆã€‚åŒæ—¶é¢„æµ‹äº§å“IDä¸º '02.88.000.00488'ï¼Œæ•°é‡ä¸º 100 çš„äº¤ä»˜æ—¶é—´ã€‚"

# ==================== å·¥å…·å‡½æ•°å®šä¹‰ ====================

def delivery_prediction_get(
    product_id: str = "02.88.000.00488",
    quantity: int = 100,
) -> str:
    """
    ä½¿ç”¨ GET æ–¹æ³•æŸ¥è¯¢äº§å“äº¤ä»˜é¢„æµ‹ä¿¡æ¯
    """
    API_BASE_URL = "http://192.168.2.168:8000/api/SaleAgent/DeliveryPrediction"
    
    try:
        params = {"product_id": product_id, "quantity": quantity}
        response = requests.get(API_BASE_URL, params=params, timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            return f"âœ… äº¤ä»˜é¢„æµ‹æŸ¥è¯¢æˆåŠŸï¼ˆGETæ–¹æ³•ï¼‰:\näº§å“ID: {product_id}\næ•°é‡: {quantity}\nç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)}"
        else:
            return f"âŒ GET è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}\né”™è¯¯ä¿¡æ¯: {response.text}"
    except requests.exceptions.Timeout:
        return f"âŒ è¯·æ±‚è¶…æ—¶ï¼šæ— æ³•è¿æ¥åˆ°äº¤ä»˜é¢„æµ‹æœåŠ¡"
    except requests.exceptions.ConnectionError:
        return f"âŒ è¿æ¥é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ {API_BASE_URL}"
    except Exception as e:
        return f"âŒ å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"


def ragflow_retrieval(
    question: str,
) -> str:
    """
    ç›´æ¥è°ƒç”¨ RAGFlow API æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
    """
    # RAGFlow API é…ç½®ï¼ˆæ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰
    RAGFLOW_API_BASE = "http://192.168.2.168:9222"
    RAGFLOW_API_KEY = ""  # å¦‚æœéœ€è¦è®¤è¯ï¼Œåœ¨è¿™é‡Œè®¾ç½®
    
    try:
        # 1. è·å–æ•°æ®é›†åˆ—è¡¨
        datasets_url = f"{RAGFLOW_API_BASE}/api/datasets"
        headers = {"Content-Type": "application/json"}
        if RAGFLOW_API_KEY:
            headers["Authorization"] = f"Bearer {RAGFLOW_API_KEY}"
        
        response = requests.get(datasets_url, headers=headers, params={"page": 1, "page_size": 1000}, timeout=10)
        
        if response.status_code != 200:
            return json.dumps(
                {"error": f"è·å–æ•°æ®é›†å¤±è´¥: {response.status_code} - {response.text}", "question": question},
                ensure_ascii=False,
            )
        
        datasets = response.json().get("data", [])
        dataset_ids = []
        for ds in datasets:
            dataset_id = ds.get("id") or ds.get("dataset_id") or ds.get("_id")
            if dataset_id:
                dataset_ids.append(str(dataset_id))
        
        if not dataset_ids:
            return json.dumps(
                {"error": "æ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†", "question": question},
                ensure_ascii=False,
            )
        
        # 2. æ‰§è¡Œæ£€ç´¢
        retrieve_url = f"{RAGFLOW_API_BASE}/api/chunks/retrieve"
        retrieve_data = {
            "question": question,
            "dataset_ids": dataset_ids,
            "page": 1,
            "page_size": 10,
            "similarity_threshold": 0.2,
            "vector_similarity_weight": 0.3,
            "top_k": 1024,
            "keyword": False,
            "highlight": False,
        }
        
        response = requests.post(retrieve_url, json=retrieve_data, headers=headers, timeout=30)
        
        if response.status_code != 200:
            return json.dumps(
                {"error": f"æ£€ç´¢å¤±è´¥: {response.status_code} - {response.text}", "question": question},
                ensure_ascii=False,
            )
        
        result = response.json()
        documents = result.get("data", {}).get("documents", [])
        scores = result.get("data", {}).get("scores", [])
        
        return json.dumps(
            {
                "question": question,
                "total": len(documents),
                "documents": documents[:3],  # åªè¿”å›å‰3ä¸ªç»“æœ
                "scores": scores[:3] if scores else [],
            },
            ensure_ascii=False,
            indent=2,
        )
    except requests.exceptions.Timeout:
        return json.dumps(
            {"error": "è¯·æ±‚è¶…æ—¶ï¼šæ— æ³•è¿æ¥åˆ°RAGFlowæœåŠ¡", "question": question},
            ensure_ascii=False,
        )
    except requests.exceptions.ConnectionError:
        return json.dumps(
            {"error": f"è¿æ¥é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ°RAGFlowæœåŠ¡å™¨ {RAGFLOW_API_BASE}", "question": question},
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"RAGFlowæ£€ç´¢å¤±è´¥: {e}", exc_info=True)
        return json.dumps(
            {"error": str(e), "question": question}, ensure_ascii=False
        )


# ==================== Mock å¯¹è±¡ ====================

def create_mock_request() -> Request:
    """åˆ›å»ºæ¨¡æ‹Ÿçš„ FastAPI Request å¯¹è±¡ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸éœ€è¦çœŸå® appï¼‰"""
    mock_request = MagicMock(spec=Request)
    mock_request.app = MagicMock()
    mock_request.app.state = MagicMock()
    mock_request.app.state.config = MagicMock()
    mock_request.app.state.config.TOOL_SERVER_CONNECTIONS = []
    mock_request.app.state.MODELS = {}
    mock_request.app.state.config.OPENAI_API_BASE_URLS = []
    mock_request.app.state.config.OPENAI_API_KEYS = []
    return mock_request


def create_mock_user(user_id: str = "test_user_1", name: str = "Test User", email: str = "test@example.com") -> UserModel:
    """åˆ›å»ºæ¨¡æ‹Ÿçš„ UserModel å¯¹è±¡"""
    import time
    current_time = int(time.time())
    
    return UserModel(
        id=user_id,
        name=name,
        email=email,
        role="user",
        profile_image_url="/user.png",
        last_active_at=current_time,
        updated_at=current_time,
        created_at=current_time,
        settings=UserSettings()
    )


async def simple_event_emitter(event: dict):
    """ç®€å•çš„äº‹ä»¶å‘å°„å™¨ï¼Œç”¨äºæµ‹è¯•"""
    event_type = event.get("type")
    data = event.get("data", {})
    
    if event_type == "status":
        action = data.get("action")
        node_id = data.get("node_id")
        node_label = data.get("node_label", node_id)
        
        if action == "agent_node_start":
            logger.info(f"ğŸš€ èŠ‚ç‚¹å¼€å§‹: {node_label} ({node_id})")
        elif action == "agent_node_end":
            elapsed_ms = data.get("elapsed_ms", 0)
            logger.info(f"âœ… èŠ‚ç‚¹å®Œæˆ: {node_label} ({node_id}) - è€—æ—¶: {elapsed_ms:.2f}ms")


# ==================== è‡ªå®šä¹‰å·¥å…·èŠ‚ç‚¹ ====================

class CustomToolNode:
    """è‡ªå®šä¹‰å·¥å…·èŠ‚ç‚¹ï¼Œç›´æ¥è°ƒç”¨å·¥å…·å‡½æ•°"""
    
    def __init__(self, node_id: str, config: Dict[str, Any], tool_function: callable):
        self.node_id = node_id
        self.config = config
        self._tool_function = tool_function
    
    def _get_input_value(self, state, port_key: str, default: Any) -> Any:
        """è·å–è¾“å…¥å€¼"""
        bindings = self.config.get("input_bindings", {})
        binding = bindings.get(port_key)
        
        if binding and "." in binding:
            source_node_id, source_port = binding.split(".", 1)
            source_msg = state.messages.get(source_node_id, {}).get(source_port)
            if source_msg:
                return source_msg.payload
        
        return default
    
    async def execute(self, state):
        """æ‰§è¡Œå·¥å…·å‡½æ•°"""
        from open_webui.agent.state import Message
        
        try:
            # è·å–å‚æ•°
            tool_params = self.config.get("tool_params", {}).copy()
            question_input = self._get_input_value(state, "question", None)
            if question_input and isinstance(question_input, str):
                if "question" not in tool_params:
                    tool_params["question"] = question_input
            
            # è·å– request å’Œ user
            request = getattr(state, "_request", None)
            user = getattr(state, "_user", None)
            
            # è¿‡æ»¤å‚æ•°ï¼Œåªä¼ é€’å‡½æ•°æ¥å—çš„å‚æ•°
            import inspect
            if self._tool_function is None:
                raise ValueError(f"å·¥å…·å‡½æ•°æœªå®šä¹‰: {self.node_id}")
            
            sig = inspect.signature(self._tool_function)
            accepted_params = list(sig.parameters.keys())
            
            # è¿‡æ»¤å‚æ•°ï¼Œåªä¼ é€’å‡½æ•°æ¥å—çš„å‚æ•°
            filtered_params = {}
            for param_name, param_value in tool_params.items():
                if param_name in accepted_params:
                    filtered_params[param_name] = param_value
            
            # è°ƒç”¨å·¥å…·å‡½æ•°ï¼ˆåŒæ­¥å‡½æ•°ï¼Œä¸éœ€è¦ awaitï¼‰
            if asyncio.iscoroutinefunction(self._tool_function):
                result = await self._tool_function(**filtered_params)
            else:
                result = self._tool_function(**filtered_params)
            
            # ä¿å­˜ç»“æœ
            if not state.messages.get(self.node_id):
                state.messages[self.node_id] = {}
            state.messages[self.node_id]["result"] = Message(
                type="text",
                payload=result
            )
            
            state.execution_path.append(self.node_id)
            return state
            
        except Exception as e:
            logger.error(f"å·¥å…·èŠ‚ç‚¹ {self.node_id} æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            if not state.messages.get(self.node_id):
                state.messages[self.node_id] = {}
            state.messages[self.node_id]["error"] = Message(
                type="error",
                payload=str(e)
            )
            state.execution_path.append(self.node_id)
            return state


# ==================== æµ‹è¯•ä¸»å‡½æ•° ====================

async def test_agent_mode():
    """æµ‹è¯• Agent æ¨¡å¼"""
    logger.info("=" * 80)
    logger.info("å¼€å§‹æµ‹è¯• Agent æ¨¡å¼ï¼ˆç›´æ¥è°ƒç”¨å·¥å…·å‡½æ•°ï¼‰")
    logger.info("=" * 80)
    logger.info(f"å›ºå®šé—®é¢˜: {FIXED_QUESTION}")
    logger.info("\nå·¥å…·åˆ—è¡¨:")
    logger.info("  1. ragflow_retrieval - æ£€ç´¢çŸ¥è¯†åº“ä¸­çš„ç›¸å…³ä¿¡æ¯")
    logger.info("  2. delivery_prediction_get - æŸ¥è¯¢äº§å“äº¤ä»˜é¢„æµ‹")
    
    # åˆ›å»º mock å¯¹è±¡
    user = create_mock_user()
    request = create_mock_request()
    
    logger.info(f"\nä½¿ç”¨ç”¨æˆ·: {user.name} ({user.id})")
    logger.info("\n" + "=" * 80)
    logger.info("å¼€å§‹æ‰§è¡Œ Agent å·¥ä½œæµ...")
    logger.info("=" * 80 + "\n")
    
    try:
        # æ„å»ºå·¥ä½œæµèŠ‚ç‚¹
        nodes = []
        connections = []
        
        # 1. è¾“å…¥èŠ‚ç‚¹
        input_node_id = "input_1"
        nodes.append({
            "id": input_node_id,
            "type": "input",
            "config": {
                "user_input": FIXED_QUESTION
            }
        })
        
        # 2. å·¥å…·èŠ‚ç‚¹1: ragflow_retrieval
        tool1_node_id = "tool_1"
        nodes.append({
            "id": tool1_node_id,
            "type": "custom_tool",
            "config": {
                "tool_name": "ragflow_retrieval",
                "tool_params": {
                    "question": "é›¶å·®äº‘æ§å…³èŠ‚é‡å¤å®šä½ç²¾åº¦"
                },
                "input_bindings": {
                    "question": f"{input_node_id}.user"
                }
            },
            "tool_function": ragflow_retrieval  # ç›´æ¥ä¼ é€’å‡½æ•°
        })
        connections.append({
            "from": input_node_id,
            "to": tool1_node_id,
            "type": "unidirectional"
        })
        
        # 3. å·¥å…·èŠ‚ç‚¹2: delivery_prediction_get
        tool2_node_id = "tool_2"
        nodes.append({
            "id": tool2_node_id,
            "type": "custom_tool",
            "config": {
                "tool_name": "delivery_prediction_get",
                "tool_params": {
                    "product_id": "02.88.000.00488",
                    "quantity": 100
                }
            },
            "tool_function": delivery_prediction_get  # ç›´æ¥ä¼ é€’å‡½æ•°
        })
        connections.append({
            "from": tool1_node_id,
            "to": tool2_node_id,
            "type": "unidirectional"
        })
        
        # 4. LLMèŠ‚ç‚¹
        llm_node_id = "llm_1"
        nodes.append({
            "id": llm_node_id,
            "type": "llm",
            "config": {
                "model": "gpt-4",
                "temperature": 0.7,
                "max_tokens": 20000,
                "prompt_template": """ä½ æ˜¯Agentçš„å®¡æ ¸å’Œæ‰§è¡Œå®˜ï¼Œè´Ÿè´£å°†å·¥å…·æ‰§è¡Œçš„ç»“æœè¿›è¡Œå®¡æ ¸ï¼Œå®¡æ ¸é€šè¿‡å¯ä»¥ç›´æ¥æ¨é€æ¶ˆæ¯ã€‚

ç”¨æˆ·é—®é¢˜: {question}

å·¥å…·æ‰§è¡Œç»“æœ:
{tool_results}

è¯·åŸºäºå·¥å…·ç»“æœï¼Œç»™å‡ºå®Œæ•´ã€å‡†ç¡®çš„å›ç­”ã€‚ç¡®ä¿å†…å®¹æœ‰æ•ˆä¸”æ˜“äºç†è§£ã€‚
"""
            }
        })
        connections.append({
            "from": tool2_node_id,
            "to": llm_node_id,
            "type": "unidirectional"
        })
        
        # 5. è¾“å‡ºèŠ‚ç‚¹
        output_node_id = "output_1"
        nodes.append({
            "id": output_node_id,
            "type": "output",
            "config": {
                "input_bindings": {
                    "answer": f"{llm_node_id}.answer"
                }
            }
        })
        connections.append({
            "from": llm_node_id,
            "to": output_node_id,
            "type": "unidirectional"
        })
        
        # ä¿®æ”¹ execute_workflow ä»¥æ”¯æŒè‡ªå®šä¹‰å·¥å…·èŠ‚ç‚¹
        # è¿™é‡Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨åˆ›å»ºå›¾ï¼Œå› ä¸ºéœ€è¦æ›¿æ¢è‡ªå®šä¹‰å·¥å…·èŠ‚ç‚¹
        from langgraph.graph import StateGraph, END, START
        from langgraph.checkpoint.memory import MemorySaver
        from open_webui.agent.state import WorkflowState, Message
        from open_webui.agent.nodes import InputNode, LLMNode, OutputNode
        from open_webui.agent.graph import WorkflowGraphState, _convert_to_langgraph_state, _convert_from_langgraph_state
        import time
        
        # åˆ›å»ºå›¾
        workflow = StateGraph(WorkflowGraphState)
        
        # åˆ›å»ºèŠ‚ç‚¹å®ä¾‹
        node_instances = {}
        for node_data in nodes:
            node_id = node_data["id"]
            node_type = node_data["type"]
            node_config = node_data.get("config", {})
            
            if node_type == "input":
                node_instance = InputNode(node_id, node_config)
            elif node_type == "custom_tool":
                # ä½¿ç”¨è‡ªå®šä¹‰å·¥å…·èŠ‚ç‚¹
                tool_function = node_data.get("tool_function")
                node_instance = CustomToolNode(node_id, node_config, tool_function)
            elif node_type == "llm":
                node_instance = LLMNode(node_id, node_config)
            elif node_type == "output":
                node_instance = OutputNode(node_id, node_config)
            else:
                logger.warning(f"æœªçŸ¥èŠ‚ç‚¹ç±»å‹: {node_type}, è·³è¿‡èŠ‚ç‚¹ {node_id}")
                continue
            
            node_instances[node_id] = node_instance
            
            # åˆ›å»ºèŠ‚ç‚¹å‡½æ•°ï¼ˆä½¿ç”¨é—­åŒ…æ•è·å˜é‡ï¼‰
            def make_node_func(nid, ntype, nconfig, ninst):
                async def node_func(state: WorkflowGraphState) -> WorkflowGraphState:
                    t0 = time.time()
                    workflow_state = _convert_from_langgraph_state(state)
                    workflow_state._request = request
                    workflow_state._user = user
                    
                    # å‘é€å¼€å§‹äº‹ä»¶
                    try:
                        await simple_event_emitter({
                            "type": "status",
                            "data": {
                                "action": "agent_node_start",
                                "node_id": nid,
                                "node_type": ntype,
                                "node_label": nconfig.get("tool_name", nid),
                                "done": False,
                            },
                        })
                    except:
                        pass
                    
                    # æ‰§è¡ŒèŠ‚ç‚¹
                    workflow_state = await ninst.execute(workflow_state)
                    workflow_state.timings[f"node_{nid}"] = (time.time() - t0) * 1000
                    
                    # å‘é€ç»“æŸäº‹ä»¶
                    try:
                        await simple_event_emitter({
                            "type": "status",
                            "data": {
                                "action": "agent_node_end",
                                "node_id": nid,
                                "node_type": ntype,
                                "node_label": nconfig.get("tool_name", nid),
                                "elapsed_ms": workflow_state.timings.get(f"node_{nid}"),
                                "done": False,
                            },
                        })
                    except:
                        pass
                    
                    return _convert_to_langgraph_state(workflow_state)
                return node_func
            
            workflow.add_node(node_id, make_node_func(node_id, node_type, node_config, node_instance))
        
        # æ·»åŠ è¾¹
        workflow.add_edge(START, input_node_id)
        for conn in connections:
            if conn.get("type") == "unidirectional":
                workflow.add_edge(conn["from"], conn["to"])
        workflow.add_edge(output_node_id, END)
        
        # ç¼–è¯‘å¹¶æ‰§è¡Œ
        checkpoint = MemorySaver()
        app = workflow.compile(checkpointer=checkpoint)
        
        initial_state = WorkflowGraphState(
            messages={},
            execution_path=[],
            question=FIXED_QUESTION,
            start_time=time.time(),
            timings={},
            retrieved_context=None,
            llm_output=None,
            total=0,
            documents=[],
            scores=[],
        )
        
        config = {"configurable": {"thread_id": "1"}}
        final_state = None
        
        async for state in app.astream(initial_state, config):
            if isinstance(state, dict) and state:
                final_state = list(state.values())[-1]
        
        # è½¬æ¢ä¸º WorkflowState
        if final_state:
            workflow_state = _convert_from_langgraph_state(final_state)
            if workflow_state.start_time:
                workflow_state.timings["total"] = (time.time() - workflow_state.start_time) * 1000
        
        logger.info("\n" + "=" * 80)
        logger.info("Agent æ‰§è¡Œå®Œæˆ")
        logger.info("=" * 80)
        
        # æ˜¾ç¤ºç»“æœ
        logger.info("\nğŸ“ Agent å›ç­”:")
        logger.info("-" * 80)
        logger.info(workflow_state.llm_output or "æ— å›ç­”")
        logger.info("-" * 80)
        
        # æ˜¾ç¤ºå·¥å…·è°ƒç”¨ç»“æœ
        tool_calls = []
        for node_id in workflow_state.execution_path:
            if node_id.startswith("tool_"):
                tool_messages = workflow_state.messages.get(node_id, {})
                tool_result_msg = tool_messages.get("result")
                tool_error_msg = tool_messages.get("error")
                if tool_result_msg:
                    tool_calls.append({
                        "tool_id": node_id,
                        "result": tool_result_msg.payload,
                        "success": True
                    })
                elif tool_error_msg:
                    tool_calls.append({
                        "tool_id": node_id,
                        "result": tool_error_msg.payload,
                        "success": False
                    })
        
        if tool_calls:
            logger.info(f"\nğŸ”§ å·¥å…·è°ƒç”¨ç»“æœ ({len(tool_calls)} ä¸ª):")
            for idx, tool_call in enumerate(tool_calls, 1):
                tool_id = tool_call.get("tool_id", "æœªçŸ¥")
                success = tool_call.get("success", False)
                result_data = tool_call.get("result", {})
                
                status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
                logger.info(f"\n  {idx}. {tool_id} - {status}")
                logger.info(f"     ç»“æœ: {str(result_data)[:500]}...")
        
        # æ˜¾ç¤ºæ‰§è¡Œæ­¥éª¤
        steps = []
        for nid in workflow_state.execution_path:
            for node_data in nodes:
                if node_data["id"] == nid:
                    ntype = node_data.get("type")
                    label = node_data.get("config", {}).get("tool_name", nid)
                    if ntype == "input":
                        label = "Input"
                    elif ntype == "llm":
                        label = "LLM"
                    elif ntype == "output":
                        label = "Output"
                    steps.append({"id": nid, "label": label})
                    break
        
        if steps:
            logger.info(f"\nğŸ“Š æ‰§è¡Œæ­¥éª¤ ({len(steps)} ä¸ª):")
            for idx, step in enumerate(steps, 1):
                step_id = step.get("id", "æœªçŸ¥")
                step_label = step.get("label", step_id)
                logger.info(f"  {idx}. {step_label} ({step_id})")
        
        # æ˜¾ç¤ºæ‰§è¡Œæ—¶é—´
        if workflow_state.timings:
            logger.info(f"\nâ±ï¸  æ‰§è¡Œæ—¶é—´:")
            for key, value in workflow_state.timings.items():
                logger.info(f"  {key}: {value:.2f}ms")
        
        logger.info("\n" + "=" * 80)
        logger.info("æµ‹è¯•å®Œæˆ")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"\nâŒ Agent æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        raise


async def main():
    """ä¸»å‡½æ•°"""
    await test_agent_mode()


if __name__ == "__main__":
    asyncio.run(main())
