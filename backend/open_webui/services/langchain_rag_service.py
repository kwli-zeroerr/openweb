"""
åŸºäºLangChainçš„RAGæœåŠ¡
æ¶µç›–ï¼šåˆ†æ®µã€å‘é‡åŒ–ã€æ£€ç´¢ã€LLMè°ƒç”¨
"""
import asyncio
import logging
from typing import List, Optional, Dict, Any, Tuple
from dataclasses import dataclass
from fastapi import Request

from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_core.documents import Document

logger = logging.getLogger(__name__)

@dataclass
class LangChainRAGQuery:
    """LangChain RAGæŸ¥è¯¢"""
    query: str
    collection_name: str
    top_k: int = 5
    use_reranking: bool = True
    mode: str = "hybrid"  # vector, bm25, hybrid
    bm25_weight: Optional[float] = None  # æ··åˆæ£€ç´¢æ—¶BM25æƒé‡ï¼ˆ0-1ï¼‰ï¼Œä¸ºç©ºåˆ™ç”¨é»˜è®¤

@dataclass
class LangChainRAGResult:
    """LangChain RAGç»“æœ"""
    query: str
    answer: str
    documents: List[Document]
    retrieval_scores: List[float]
    rerank_scores: Optional[List[float]] = None
    method: str = "hybrid"
    retrieval_time: Optional[float] = None  # æ£€ç´¢è€—æ—¶ï¼ˆç§’ï¼‰

class LangChainRAGService:
    """åŸºäºLangChainçš„å®Œæ•´RAGæœåŠ¡"""
    
    def __init__(self, embedding_model: str = None, embedding_function=None):
        """
        åˆå§‹åŒ–RAGæœåŠ¡
        
        Args:
            embedding_model: embeddingæ¨¡å‹åç§°ï¼ˆå·²å¿½ç•¥ï¼Œå¼ºåˆ¶ä½¿ç”¨vLLMæœåŠ¡ï¼‰
            embedding_function: é¢„é…ç½®çš„embeddingå‡½æ•°ï¼ˆå·²å¿½ç•¥ï¼Œå¼ºåˆ¶ä½¿ç”¨vLLMæœåŠ¡ï¼‰
        """
        # vLLM Embedding æœåŠ¡åœ°å€ç”±é…ç½®æä¾›ï¼Œä¿ç•™é»˜è®¤
        self._vllm_api_url = None  # ç”± get_embeddings_client åŠ¨æ€è§£æ
        self._vllm_base_url = None
        
        # æ ‡è®°ï¼šå¼ºåˆ¶ä½¿ç”¨vLLMæœåŠ¡ï¼Œå¿½ç•¥å…¶ä»–é…ç½®
        self._use_external_embedding = True
        self._embedding_function = None  # ä¸ä½¿ç”¨å¤–éƒ¨å‡½æ•°
        self.embeddings = None  # ä¸ä½¿ç”¨LangChainçš„HuggingFaceEmbeddings
        self._embedding_model = "vllm_embedding_service"  # ä¿ç•™å±æ€§ä»¥é¿å…AttributeErrorï¼Œå€¼è¡¨ç¤ºä½¿ç”¨vLLMæœåŠ¡
        
        # ä»…åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶ä» request é…ç½®è§£æ URLï¼ˆè§ _get_clientï¼‰
        
        # å‘é‡å­˜å‚¨ï¼ˆå†…å­˜ï¼‰
        self.vector_stores: Dict[str, Any] = {}
        
        # LLMï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._llm = None
    
    
    def _get_client(self, request=None):
        try:
            from .embeddings_client import get_embeddings_client, _get_vllm_url_from_request
            client = get_embeddings_client(request)
            # ç¼“å­˜å±•ç¤ºç”¨ base_url
            base = _get_vllm_url_from_request(request)
            self._vllm_api_url = base
            self._vllm_base_url = f"{base}/v1/embeddings"
            return client
        except Exception as e:
            raise ValueError(f"Cannot init embeddings client: {e}")

    async def _embed_text(self, text: str, request=None) -> List[float]:
        """åµŒå…¥æ–‡æœ¬ï¼ˆç»Ÿä¸€æ¥å£ï¼‰- å¼ºåˆ¶ä½¿ç”¨vLLMæœåŠ¡"""
        try:
            client = self._get_client(request)
            return await client.embed_one(text)
        except Exception as e:
            logger.error(f"âš ï¸ vLLM Embedding API failed (url={self._vllm_base_url}): {e}", exc_info=True)
            raise ValueError(f"vLLM Embedding API failed: {e}")
    
    async def _embed_texts_batch(self, texts: List[str], request=None) -> List[List[float]]:
        """æ‰¹é‡åµŒå…¥æ–‡æœ¬ï¼ˆç»Ÿä¸€æ¥å£ï¼‰- å¼ºåˆ¶ä½¿ç”¨vLLMæœåŠ¡"""
        try:
            client = self._get_client(None)
            return await client.embed_batch(texts)
        except Exception as e:
            logger.error(f"âš ï¸ vLLM Embedding API batch failed (url={self._vllm_base_url}): {e}", exc_info=True)
            raise ValueError(f"vLLM Embedding API batch failed: {e}")
    
    def get_llm(self, request=None):
        """LLM åŠŸèƒ½å·²ç§»é™¤ï¼Œè¿”å› text_onlyã€‚"""
        return "text_only"
    
    def chunk_markdown(
        self, 
        text: str, 
        chunk_size: int = 1000, 
        chunk_overlap: int = 200,
        use_header_splitter: bool = True
    ) -> List[Document]:
        """
        Markdownæ–‡æ¡£åˆ†æ®µ
        
        Args:
            text: Markdownæ–‡æœ¬
            chunk_size: åˆ†æ®µå¤§å°
            chunk_overlap: é‡å å¤§å°
            use_header_splitter: æ˜¯å¦ä½¿ç”¨æ ‡é¢˜åˆ†å‰²å™¨
        
        Returns:
            åˆ†æ®µåçš„Documentåˆ—è¡¨
        """
        if use_header_splitter:
            # æŒ‰æ ‡é¢˜å±‚çº§åˆ†å‰²
            headers_to_split_on = [
                ("#", "Header 1"),
                ("##", "Header 2"),
                ("###", "Header 3"),
            ]
            splitter = MarkdownHeaderTextSplitter(
                headers_to_split_on=headers_to_split_on,
                strip_headers=False
            )
            
            # åˆ†å‰²
            splits = splitter.split_text(text)
            
            # å¦‚æœæ®µè½å¤ªé•¿ï¼Œå†é€’å½’åˆ†å‰²
            if any(len(split.page_content) > chunk_size for split in splits):
                recursive_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap,
                    separators=["\n\n", "\n", "ã€‚", " ", ""]
                )
                final_splits = []
                for split in splits:
                    if len(split.page_content) > chunk_size:
                        sub_splits = recursive_splitter.split_documents([split])
                        for sub in sub_splits:
                            final_splits.append(Document(
                                page_content=sub.page_content,
                                metadata={**split.metadata, **sub.metadata}
                            ))
                    else:
                        final_splits.append(split)
                return final_splits
            
            return splits
        else:
            # çº¯æ–‡æœ¬åˆ†æ®µ
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=["\n\n", "\n", "ã€‚", " ", ""]
            )
            return text_splitter.create_documents([text])
    
    async def load_collection(
        self, 
        collection_name: str, 
        documents: List[Document],
        use_faiss: bool = False
    ):
        """
        åŠ è½½æ–‡æ¡£åˆ°æ£€ç´¢ç´¢å¼•
        
        Args:
            collection_name: é›†åˆåç§°
            documents: æ–‡æ¡£åˆ—è¡¨
            use_faiss: æ˜¯å¦ä½¿ç”¨FAISSï¼ˆéœ€è¦å®‰è£…ï¼‰
        """
        # âš ï¸ è®°å½•åŠ è½½ä¿¡æ¯ï¼ˆå¼ºåˆ¶ä½¿ç”¨vLLMæœåŠ¡ï¼‰
        logger.info(f"Loading collection '{collection_name}' with {len(documents)} documents")
        
        # åˆ›å»ºå‘é‡å­˜å‚¨ï¼ˆä»…å†…å­˜å­˜å‚¨ï¼Œä½¿ç”¨ vLLM æ‰¹é‡å‘é‡åŒ–ï¼‰
        # æ‰¹é‡å‘é‡åŒ–
        texts = [doc.page_content for doc in documents]
        
        try:
            vectors = await self._embed_texts_batch(texts)
            
            # éªŒè¯å‘é‡æ•°é‡
            if len(vectors) != len(documents):
                logger.warning(f"âš ï¸ å‘é‡æ•°é‡ä¸åŒ¹é…: {len(vectors)} vectors for {len(documents)} documents")
                # åªä¿ç•™æˆåŠŸå‘é‡åŒ–çš„æ–‡æ¡£
                documents = documents[:len(vectors)]
                vectors = vectors[:len(documents)]
            
            # è®°å½•å‘é‡ç»´åº¦ï¼ˆå¯é€‰ï¼‰
        except Exception as e:
            logger.error(f"âŒ vLLMæ‰¹é‡å‘é‡åŒ–å¤±è´¥: {e}")
            vectors = []
        
        if len(vectors) != len(documents):
            logger.warning(f"Vector count mismatch: {len(vectors)} vectors for {len(documents)} documents")
            # åªä¿ç•™æˆåŠŸå‘é‡åŒ–çš„æ–‡æ¡£
            documents = [doc for i, doc in enumerate(documents) if i < len(vectors)]
        
        if len(vectors) == 0:
            logger.error(f"No valid vectors generated for collection {collection_name}")
            self.vector_stores[collection_name] = {
                "docs": [],
                "vectors": []
            }
        else:
            self.vector_stores[collection_name] = {
                "docs": documents[:len(vectors)],
                "vectors": vectors
            }
        
        logger.info(f"Collection '{collection_name}' loaded: {len(documents)} docs")
    
    async def vector_search(
        self, 
        query: str, 
        collection_name: str, 
        top_k: int = 5,
        use_weighted_multi_channel: bool = True
    ) -> List[Tuple[Document, float]]:
        """çº¯å‘é‡æ£€ç´¢
        
        å¦‚æœuse_weighted_multi_channel=Trueï¼Œæ”¯æŒå¤šé€šé“åŠ æƒæ£€ç´¢ï¼ˆæ ‡é¢˜0.15ã€å†…å®¹0.7ã€é—®é¢˜0.15ï¼‰
        
        ä¼˜åŒ–ï¼šå¦‚æœå†…å­˜ä¸­æ²¡æœ‰ç¼“å­˜ï¼Œç›´æ¥ä»å‘é‡æ•°æ®åº“æ£€ç´¢ï¼Œé¿å…é‡æ–°å‘é‡åŒ–
        """
        # å‘é‡æ£€ç´¢ï¼ˆå†…å­˜æˆ–ç›´è¿å‘é‡åº“ï¼‰
        
        # âš ï¸ å¦‚æœå†…å­˜ä¸­æ²¡æœ‰ç¼“å­˜ï¼Œç›´æ¥ä»å‘é‡æ•°æ®åº“æ£€ç´¢ï¼ˆä¼˜åŒ–ï¼šé¿å…é‡æ–°å‘é‡åŒ–ï¼‰
        if collection_name not in self.vector_stores:
            # æœªç¼“å­˜æ—¶ï¼Œç›´æ¥ä»å‘é‡æ•°æ®åº“æ£€ç´¢
            return await self._vector_search_direct(collection_name, query, top_k, use_weighted_multi_channel)
        
        vector_store = self.vector_stores[collection_name]
        
        query_vector = await self._embed_text(query)
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æŸ¥è¯¢å‘é‡
        if not query_vector or len(query_vector) == 0:
            logger.error(f"Query vector is empty or None for query: {query[:50]}...")
            return []
        
        # ç®€å•å†…å­˜æœç´¢ï¼ˆæœ¬æœåŠ¡ä»…å†™å…¥ dict å­˜å‚¨ï¼‰
            docs = vector_store["docs"]
            vectors = vector_store["vectors"]
            
        if not vectors or len(vectors) == 0:
            logger.warning(f"No vectors found in collection {collection_name}")
            return []
        
        # æ£€æŸ¥å‘é‡ç»´åº¦
        query_dim = len(query_vector)
        doc_dim = len(vectors[0]) if vectors else 0
        if query_dim != doc_dim:
            logger.error(f"Vector dimension mismatch: query={query_dim}, document={doc_dim}")
            return []
            
            import numpy as np
        query_array = np.array(query_vector, dtype=np.float32)
        query_norm = np.linalg.norm(query_array)
        if query_norm == 0:
            logger.warning("Query vector is zero vector, cannot compute similarities")
            return []
        
        similarities = []
        for vec in vectors:
            vec_array = np.array(vec, dtype=np.float32)
            vec_norm = np.linalg.norm(vec_array)
            if vec_norm == 0:
                similarities.append(0.0)
            else:
                sim = float(np.dot(query_array, vec_array) / (query_norm * vec_norm))
                similarities.append(0.0 if (np.isnan(sim) or np.isinf(sim)) else sim)
            
            if use_weighted_multi_channel:
                search_limit = min(len(docs), top_k * 5)
                preliminary_ranked = sorted(zip(docs, similarities), key=lambda x: x[1], reverse=True)[:search_limit]
                preliminary_docs = [d for d, _ in preliminary_ranked]
                preliminary_scores = [s for _, s in preliminary_ranked]
                return await self._weighted_multi_channel_search(preliminary_docs, preliminary_scores, top_k)
            else:
                ranked = sorted(zip(docs, similarities), key=lambda x: x[1], reverse=True)
            results = ranked[:top_k]
        
        return results
    
    async def _vector_search_direct(
        self,
        collection_name: str,
        query: str,
        top_k: int,
        use_weighted_multi_channel: bool
    ) -> List[Tuple[Document, float]]:
        """ç›´æ¥ä»å‘é‡æ•°æ®åº“æ£€ç´¢ï¼Œä¸åŠ è½½åˆ°å†…å­˜ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰"""
        import time
        start_time = time.time()
        
        try:
            from open_webui.retrieval.vector.factory import VECTOR_DB_CLIENT
            from langchain_core.documents import Document
            
            # 0. æŸ¥æ‰¾å®é™…é›†åˆåç§°ï¼ˆå°è¯•å¤šç§æ ¼å¼ï¼‰
            actual_collection_name = None
            possible_collection_names = [
                collection_name,  # ç›´æ¥ä½¿ç”¨çŸ¥è¯†åº“ID
                f"knowledge_{collection_name}",
                f"kb_{collection_name}",
            ]
            
            for candidate_name in possible_collection_names:
                if VECTOR_DB_CLIENT.has_collection(candidate_name):
                    actual_collection_name = candidate_name
                    logger.info(f"âœ… Found collection: {actual_collection_name}")
                    break
            
            # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•åˆ—å‡ºæ‰€æœ‰é›†åˆ
            if not actual_collection_name and hasattr(VECTOR_DB_CLIENT, 'list_collections'):
                try:
                    all_collections = VECTOR_DB_CLIENT.list_collections()
                    logger.debug(f"Available collections: {all_collections}")
                    
                    # æŸ¥æ‰¾åŒ…å«çŸ¥è¯†åº“IDçš„é›†åˆ
                    for coll_name in all_collections:
                        if collection_name in coll_name or coll_name == collection_name:
                            actual_collection_name = coll_name
                            logger.info(f"âœ… Found matching collection: {actual_collection_name}")
                            break
                    
                    # å¦‚æœä»ç„¶æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨é›†åˆï¼ˆä½œä¸ºåå¤‡ï¼‰
                    if not actual_collection_name and all_collections:
                        actual_collection_name = all_collections[0]
                        logger.warning(f"âš ï¸ Using first available collection as fallback: {actual_collection_name}")
                except Exception as e:
                    logger.warning(f"Failed to list collections: {e}")
            
            if not actual_collection_name:
                logger.error(f"âŒ No collection found for '{collection_name}'. Tried: {possible_collection_names}")
                return []
            
            # 1. å‘é‡åŒ–æŸ¥è¯¢ï¼ˆä½¿ç”¨vLLMæœåŠ¡ï¼‰
            query_vector = await self._embed_text(query)
            logger.debug(f"âœ… Query vectorized via vLLM: dimension={len(query_vector) if query_vector else 0}")
            
            if not query_vector or len(query_vector) == 0:
                logger.error(f"Query vector is empty or None for query: {query[:50]}...")
                return []
            
            # 2. ç›´æ¥ä»å‘é‡æ•°æ®åº“æœç´¢ï¼ˆä½¿ç”¨å®é™…æ‰¾åˆ°çš„é›†åˆåç§°ï¼‰
            logger.debug(f"ğŸ” Searching in collection: {actual_collection_name}, query vector dimension: {len(query_vector)}")
            
            # æ£€æŸ¥å‘é‡ç»´åº¦æ˜¯å¦åŒ¹é…ï¼ˆå¦‚æœé›†åˆå·²æœ‰å‘é‡ï¼‰
            try:
                # è·å–é›†åˆä¸­çš„ç¬¬ä¸€ä¸ªå‘é‡æ¥æ£€æŸ¥ç»´åº¦
                check_result = VECTOR_DB_CLIENT.get(actual_collection_name, limit=1)
                if check_result and check_result.ids and check_result.ids[0]:
                    # å¦‚æœèƒ½è·å–åˆ°å‘é‡ï¼Œæ£€æŸ¥ç»´åº¦
                    # æ³¨æ„ï¼šæŸäº›å‘é‡DBçš„getæ–¹æ³•å¯èƒ½ä¸è¿”å›å‘é‡ï¼Œåªè¿”å›metadata
                    logger.debug(f"Collection '{actual_collection_name}' has documents, proceeding with search")
            except Exception as e:
                logger.debug(f"Could not pre-check collection: {e}")
            
            # å…ˆéªŒè¯é›†åˆç¡®å®å­˜åœ¨ä¸”å¯è®¿é—®
            try:
                if not VECTOR_DB_CLIENT.has_collection(actual_collection_name):
                    logger.error(f"âŒ Collection '{actual_collection_name}' does not exist (has_collection returned False)")
                    return []
            except Exception as e:
                logger.warning(f"Could not verify collection existence: {e}")
            
            # æ‰§è¡Œæœç´¢
            try:
                search_result = await asyncio.to_thread(
                    VECTOR_DB_CLIENT.search,
                    collection_name=actual_collection_name,
                    vectors=[query_vector],
                    limit=top_k * 5 if use_weighted_multi_channel else top_k
                )
            except Exception as e:
                logger.error(f"Vector DB search failed for collection '{actual_collection_name}': {e}", exc_info=True)
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç»´åº¦ä¸åŒ¹é…çš„é—®é¢˜
                if "dimension" in str(e).lower() or "size" in str(e).lower():
                    logger.error(f"âš ï¸ Possible dimension mismatch! Query vector: {len(query_vector)} dimensions")
                # å°è¯•è·å–é›†åˆä¿¡æ¯æ¥è¯Šæ–­é—®é¢˜
                try:
                    check_result = VECTOR_DB_CLIENT.get(actual_collection_name)
                    if check_result:
                        logger.error(f"Collection exists but search failed. Collection info: {len(check_result.ids[0]) if check_result.ids and check_result.ids[0] else 0} items")
                except:
                    pass
                return []
            
            # æ£€æŸ¥æœç´¢ç»“æœ
            if not search_result:
                logger.warning(f"Search returned None for collection '{actual_collection_name}'")
                # å¯èƒ½æ˜¯é›†åˆå­˜åœ¨ä½†æ²¡æœ‰æ•°æ®ï¼Œæˆ–get_collectionå¤±è´¥
                try:
                    check_result = VECTOR_DB_CLIENT.get(actual_collection_name)
                    if check_result and check_result.documents and check_result.documents[0]:
                        doc_count = len(check_result.documents[0])
                        logger.warning(f"Collection '{actual_collection_name}' has {doc_count} documents but search returned None")
                        logger.warning(f"âš ï¸ This might indicate the collection exists but search method failed")
                    else:
                        logger.warning(f"Collection '{actual_collection_name}' appears to be empty or inaccessible")
                except Exception as e:
                    logger.warning(f"Could not check collection contents: {e}")
                return []
            
            if not search_result.documents or not search_result.documents[0]:
                logger.warning(f"No documents in search result for collection '{actual_collection_name}'")
                return []
            
            # 3. è½¬æ¢ä¸ºDocumentæ ¼å¼
            docs = []
            scores = []
            
            for i, doc_text in enumerate(search_result.documents[0]):
                # è·å–metadata
                metadata = search_result.metadatas[0][i] if search_result.metadatas and search_result.metadatas[0] else {}
                
                # è·å–è·ç¦»å¹¶è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                distance = search_result.distances[0][i] if search_result.distances and search_result.distances[0] else 1.0
                # Chromaè·ç¦»: 0(æœ€å¥½) -> 2(æœ€å·®)ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦: 1(æœ€å¥½) -> 0(æœ€å·®)
                similarity = 1.0 - (distance / 2.0) if distance <= 2.0 else 0.0
                
                doc = Document(
                    page_content=doc_text,
                    metadata={**metadata, "distance": distance}
                )
                docs.append(doc)
                scores.append(similarity)
            
            # 4. å¦‚æœéœ€è¦å¤šé€šé“åŠ æƒæ£€ç´¢
            if use_weighted_multi_channel:
                results = await self._weighted_multi_channel_search(docs, scores, top_k)
            else:
                # æŒ‰åˆ†æ•°æ’åº
                results = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)[:top_k]
            
            # è®°å½•æ£€ç´¢æ—¶é—´
            elapsed_time = time.time() - start_time
            logger.info(f"âœ… Direct vector DB search completed in {elapsed_time:.3f}s, found {len(results)} documents")
            return results
            
        except Exception as e:
            logger.error(f"Direct vector DB search failed: {e}", exc_info=True)
            return []
    
    async def _weighted_multi_channel_search(
        self,
        docs: List[Document],
        similarities: List[float],
        top_k: int
    ) -> List[Tuple[Document, float]]:
        """å¤šé€šé“åŠ æƒæ£€ç´¢ï¼šæ ‡é¢˜0.15ã€å†…å®¹0.7ã€é—®é¢˜0.15"""
        import numpy as np
        
        # æŒ‰segment_idå’Œtypeåˆ†ç»„
        segment_scores: Dict[str, Dict[str, Tuple[Document, float, float]]] = {}  # (doc, original_score, weighted_score)
        old_data_results: List[Tuple[Document, float]] = []  # æ—§æ•°æ®ï¼ˆæ— segment_idï¼‰
        
        for doc, score in zip(docs, similarities):
            meta = doc.metadata or {}
            segment_id = meta.get("segment_id", "")
            doc_type = meta.get("type", "content")
            weight = meta.get("weight", 0.7)  # é»˜è®¤æƒé‡ï¼šæ ‡é¢˜0.15ã€å†…å®¹0.7ã€é—®é¢˜0.15
            
            # åŠ æƒåˆ†æ•°
            weighted_score = score * weight
            
            if not segment_id:
                # æ²¡æœ‰segment_idçš„æ—§æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨åŠ æƒåˆ†æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
                old_data_results.append((doc, weighted_score))
                continue
            
            if segment_id not in segment_scores:
                segment_scores[segment_id] = {}
            
            # åŒä¸€åˆ†æ®µçš„åŒä¸€ç±»å‹åªä¿ç•™æœ€é«˜åˆ†ï¼ˆæŒ‰åŠ æƒåˆ†æ•°æ¯”è¾ƒï¼Œä½†ä¿å­˜åŸå§‹åˆ†æ•°ï¼‰
            current_best = segment_scores[segment_id].get(doc_type)
            if not current_best or weighted_score > current_best[2]:  # æ¯”è¾ƒåŠ æƒåˆ†æ•°
                segment_scores[segment_id][doc_type] = (doc, score, weighted_score)  # ä¿å­˜åŸå§‹åˆ†æ•°å’ŒåŠ æƒåˆ†æ•°
        
        # åˆå¹¶åŒä¸€åˆ†æ®µçš„ä¸åŒç±»å‹ï¼Œç´¯åŠ åˆ†æ•°
        merged_results: Dict[str, Tuple[Document, float]] = {}
        for segment_id, type_scores in segment_scores.items():
            total_score = sum(weighted_score for _, _, weighted_score in type_scores.values())  # ç´¯åŠ åŠ æƒåˆ†æ•°
            # æ„å»ºå®Œæ•´çš„ä»£è¡¨Documentï¼ŒåŒ…å«æ ‡é¢˜+å†…å®¹+é—®é¢˜
            title_data = type_scores.get("title")  # (doc, original_score, weighted_score)
            content_data = type_scores.get("content")
            questions_data = type_scores.get("questions")
            
            # ä¼˜å…ˆä½¿ç”¨contentä½œä¸ºåŸºç¡€
            base_doc_data = content_data if content_data else (title_data if title_data else questions_data if questions_data else None)
            if not base_doc_data or not isinstance(base_doc_data, tuple) or len(base_doc_data) < 3:
                continue
            base_doc = base_doc_data[0]
            
            # æå–å„é€šé“çš„åŸå§‹åˆ†æ•°å’ŒåŠ æƒåˆ†æ•°ï¼ˆå®‰å…¨è®¿é—®ï¼‰
            title_original = title_data[1] if (title_data and isinstance(title_data, tuple) and len(title_data) >= 3) else None
            title_weighted = title_data[2] if (title_data and isinstance(title_data, tuple) and len(title_data) >= 3) else None
            content_original = content_data[1] if (content_data and isinstance(content_data, tuple) and len(content_data) >= 3) else None
            content_weighted = content_data[2] if (content_data and isinstance(content_data, tuple) and len(content_data) >= 3) else None
            questions_original = questions_data[1] if (questions_data and isinstance(questions_data, tuple) and len(questions_data) >= 3) else None
            questions_weighted = questions_data[2] if (questions_data and isinstance(questions_data, tuple) and len(questions_data) >= 3) else None
            
            # è·å–Documentå¯¹è±¡ç”¨äºæ„å»ºå†…å®¹ï¼ˆå®‰å…¨è®¿é—®ï¼‰
            title_doc = title_data[0] if (title_data and isinstance(title_data, tuple) and len(title_data) >= 1) else None
            content_doc = content_data[0] if (content_data and isinstance(content_data, tuple) and len(content_data) >= 1) else None
            questions_doc = questions_data[0] if (questions_data and isinstance(questions_data, tuple) and len(questions_data) >= 1) else None
            
            # æ„å»ºå®Œæ•´çš„æ–‡æœ¬å†…å®¹ï¼ˆæ ‡é¢˜ + å†…å®¹ + é—®é¢˜ï¼‰
            full_content_parts = []
            if title_doc:
                # ä»é¢åŒ…å±‘æ ¼å¼ä¸­æå–æ ‡é¢˜éƒ¨åˆ†
                title_text = title_doc.page_content  # title_doc å·²ç»æ˜¯ Document å¯¹è±¡ï¼Œä¸éœ€è¦ [0]
                if ":" in title_text:
                    full_content_parts.append(title_text.split(":", 1)[1].strip())
            if content_doc:
                content_text = content_doc.page_content  # content_doc å·²ç»æ˜¯ Document å¯¹è±¡ï¼Œä¸éœ€è¦ [0]
                if ":" in content_text:
                    full_content_parts.append(content_text.split(":", 1)[1].strip())
            if questions_doc:
                questions_text = questions_doc.page_content  # questions_doc å·²ç»æ˜¯ Document å¯¹è±¡ï¼Œä¸éœ€è¦ [0]
                if ":" in questions_text:
                    full_content_parts.append(f"é—®é¢˜ï¼ˆé€‰å¡«ï¼Œå•å…ƒæ ¼å†…ä¸€è¡Œä¸€ä¸ªï¼‰: {questions_text.split(':', 1)[1].strip()}")
            
            # æ›´æ–°metadataï¼Œæ·»åŠ å„é€šé“åˆ†æ•°ä¿¡æ¯
            updated_metadata = base_doc.metadata.copy() if base_doc.metadata else {}
            updated_metadata["channel_scores"] = {
                "title_original": title_original,
                "content_original": content_original,
                "questions_original": questions_original,
                "title_weighted": title_weighted,
                "content_weighted": content_weighted,
                "questions_weighted": questions_weighted,
            }
            
            # åˆ›å»ºæ–°çš„Documentï¼Œä½¿ç”¨å®Œæ•´å†…å®¹
            merged_doc = Document(
                page_content="\n\n".join(full_content_parts) if full_content_parts else base_doc.page_content,
                metadata=updated_metadata
            )
            merged_results[segment_id] = (merged_doc, total_score)
        
        # åˆå¹¶æ–°æ—§æ•°æ®å¹¶æ’åº
        all_results = list(merged_results.values()) + old_data_results
        ranked = sorted(all_results, key=lambda x: x[1], reverse=True)
        return ranked[:top_k]
    
    # å·²ç§»é™¤ï¼šä»å¤–éƒ¨å‘é‡åº“ç»“æœåšå¤šé€šé“åŠ æƒçš„åˆ†æ”¯
    
    # å·²ç§»é™¤ï¼šBM25 æ£€ç´¢ç›¸å…³å®ç°

    def _clean_text(self, text: str) -> str:
        """è½»é‡æ¸…æ´—ï¼šå» HTML æ ‡ç­¾ã€å¤šä½™ç©ºç™½"""
        try:
            import re
            # å»æ ‡ç­¾
            t = re.sub(r"<[^>]+>", " ", text)
            # åˆå¹¶ç©ºç™½
            t = re.sub(r"\s+", " ", t).strip()
            return t
        except Exception:
            return text
    
    # å·²ç§»é™¤ï¼šHybrid æ£€ç´¢ç›¸å…³å®ç°
    
    
    
    async def generate_answer(self, query: str, context: str, request=None) -> str:
        """LLM å·²ç§»é™¤ï¼šç›´æ¥è¿”å›ä¸Šä¸‹æ–‡ã€‚"""
        return context
    
    async def query(self, rag_query: LangChainRAGQuery, request=None) -> LangChainRAGResult:
        """
        æ‰§è¡ŒRAGæŸ¥è¯¢ï¼ˆå®Œæ•´æµç¨‹ï¼‰
        
        Args:
            rag_query: RAGæŸ¥è¯¢å¯¹è±¡
        
        Returns:
            RAGæŸ¥è¯¢ç»“æœ
        """
        import time
        retrieval_start_time = time.time()
        
        # 1. ä»…å‘é‡æ£€ç´¢
        results = await self.vector_search(
                rag_query.query, 
                rag_query.collection_name, 
                rag_query.top_k
            )
        documents = [doc for doc, score in results]
        scores = [score for doc, score in results]
        method = "vector"
        
        # è®¡ç®—æ£€ç´¢æ—¶é—´
        retrieval_time = time.time() - retrieval_start_time
        
        if not documents:
            return LangChainRAGResult(
                query=rag_query.query,
                answer="æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£",
                documents=[],
                retrieval_scores=[],
                method=method,
                retrieval_time=retrieval_time
            )
        
        # 2. é‡æ’åºå·²ç§»é™¤
        rerank_scores = None
        
        # 3. æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"æ–‡æ¡£ {i+1}:\n{doc.page_content[:500]}"
            for i, doc in enumerate(documents[:3])
        ])
        
        # 4. ç”Ÿæˆå›ç­”
        answer = await self.generate_answer(rag_query.query, context, request)
        
        return LangChainRAGResult(
            query=rag_query.query,
            answer=answer,
            documents=documents,
            retrieval_scores=scores[:len(documents)],
            rerank_scores=rerank_scores,
            method=method,
            retrieval_time=retrieval_time
        )

# å…¨å±€å®ä¾‹ç¼“å­˜ï¼šåŸºäºembeddingé…ç½®çš„é”®æ¥ç¼“å­˜å®ä¾‹
# è¿™æ ·å¯ä»¥ç¡®ä¿ç›¸åŒçš„embeddingé…ç½®ä½¿ç”¨åŒä¸€ä¸ªå®ä¾‹ï¼ˆå…±äº«vector_storesï¼‰
_langchain_rag_service_instances: Dict[str, LangChainRAGService] = {}

def _get_instance_key(request: Request = None) -> str:
    """ç”Ÿæˆå®ä¾‹ç¼“å­˜é”®ï¼ŒåŸºäºembeddingé…ç½®"""
    if request is None:
        return "default"
    
    # åŸºäºembedding engineå’Œmodelç”Ÿæˆé”®
    embedding_engine = getattr(request.app.state.config, 'RAG_EMBEDDING_ENGINE', '')
    embedding_model = getattr(request.app.state.config, 'RAG_EMBEDDING_MODEL', '')
    
    # æ£€æŸ¥æ˜¯å¦æœ‰EMBEDDING_FUNCTION
    has_ef = hasattr(request.app.state, 'EMBEDDING_FUNCTION') and request.app.state.EMBEDDING_FUNCTION is not None
    has_local_ef = hasattr(request.app.state, 'ef') and request.app.state.ef is not None
    
    if has_ef:
        return f"external_ef_{embedding_engine}_{embedding_model}"
    elif has_local_ef:
        return f"local_ef_{embedding_model}"
    else:
        return f"default_{embedding_model}"

def get_langchain_rag_service(request: Request = None) -> LangChainRAGService:
    """è·å–LangChainRAGServiceå®ä¾‹
    âš ï¸ å¼ºåˆ¶ä½¿ç”¨ vLLM Embedding æœåŠ¡ (192.168.1.232:8010)ï¼Œå¿½ç•¥æ‰€æœ‰å¤–éƒ¨é…ç½®
    """
    global _langchain_rag_service_instances
    
    # âš ï¸ å¼ºåˆ¶ä½¿ç”¨å›ºå®šçš„å®ä¾‹é”®ï¼Œç¡®ä¿æ‰€æœ‰å®ä¾‹éƒ½ä½¿ç”¨vLLMæœåŠ¡
    instance_key = "vllm_forced_service"
    
    # å¦‚æœå·²ç¼“å­˜ï¼Œç›´æ¥è¿”å›
    if instance_key in _langchain_rag_service_instances:
        logger.debug(f"âœ… LangChainRAGService å¤ç”¨å·²ç¼“å­˜å®ä¾‹ (å¼ºåˆ¶ä½¿ç”¨vLLMæœåŠ¡)")
        return _langchain_rag_service_instances[instance_key]
    
    # âš ï¸ å¼ºåˆ¶åˆ›å»ºä½¿ç”¨vLLMæœåŠ¡çš„æ–°å®ä¾‹ï¼Œå¿½ç•¥æ‰€æœ‰å¤–éƒ¨é…ç½®
    logger.info(f"âš ï¸ å¼ºåˆ¶åˆ›å»º LangChainRAGService å®ä¾‹ï¼Œä½¿ç”¨ vLLM Embedding æœåŠ¡")
    logger.info(f"   vLLM API: http://192.168.1.232:8010/v1/embeddings")
    logger.info(f"   æ‰€æœ‰å‘é‡åŒ–å’Œå‘é‡æ£€ç´¢éƒ½å°†é€šè¿‡æ­¤æœåŠ¡è¿›è¡Œï¼")
    
    # å¿½ç•¥ request ä¸­çš„æ‰€æœ‰ embedding é…ç½®ï¼Œå¼ºåˆ¶ä½¿ç”¨vLLM
    service = LangChainRAGService()
    _langchain_rag_service_instances[instance_key] = service
    logger.info(f"âœ… LangChainRAGService åˆ›å»ºå®Œæˆ (å¼ºåˆ¶ä½¿ç”¨vLLMæœåŠ¡)")
    return service

# é»˜è®¤å®ä¾‹ï¼ˆå‘åå…¼å®¹ï¼‰
langchain_rag_service = LangChainRAGService()

