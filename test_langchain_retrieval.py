#!/usr/bin/env python3
"""
ä½¿ç”¨LangChainè¿›è¡Œç®€å•çš„æ–‡æ¡£æ£€ç´¢æµ‹è¯•
ç›´æ¥ä»ChromaåŠ è½½æ•°æ®ï¼Œç„¶åç”¨LangChainæ£€ç´¢
"""
import asyncio
import sys
sys.path.insert(0, '/home/zeroerr-ai72/openwebui-zeroerr/backend')

from open_webui.retrieval.vector.factory import VECTOR_DB_CLIENT
from open_webui.services.langchain_rag_service import langchain_rag_service
from langchain_core.documents import Document
import numpy as np
import asyncio

def cosine_similarity(a, b):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def load_documents_from_chroma(collection_name: str = "test3"):
    """ä»ChromaåŠ è½½æ–‡æ¡£"""
    print(f"\nğŸ“‚ ä»ChromaåŠ è½½é›†åˆ: {collection_name}")
    
    result = VECTOR_DB_CLIENT.get(collection_name)
    
    if not result or not result.documents or not result.documents[0]:
        print(f"âš ï¸  é›†åˆä¸ºç©º")
        return []
    
    documents = []
    for text, metadata in zip(result.documents[0], result.metadatas[0]):
        doc = Document(page_content=text, metadata=metadata)
        documents.append(doc)
    
    print(f"âœ… åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    return documents

async def vector_search(query, documents, top_k=5):
    """å‘é‡æ£€ç´¢"""
    print(f"\nğŸ”µ å‘é‡æ£€ç´¢:")
    
    # å‘é‡åŒ–æŸ¥è¯¢
    query_vector = await langchain_rag_service._embed_text(query)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    scores = []
    for doc in documents:
        doc_vector = await langchain_rag_service._embed_text(doc.page_content)
        score = cosine_similarity(query_vector, doc_vector)
        scores.append(score)
    
    # æ’åºå–top_k
    ranked = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
    
    for i, (doc, score) in enumerate(ranked[:top_k], 1):
        print(f"   {i}. [ç›¸ä¼¼åº¦: {score:.3f}] {doc.page_content[:80]}...")
    
    return [doc for doc, score in ranked[:top_k]]

def bm25_search(query, documents, top_k=5):
    """BM25å…¨æ–‡æ£€ç´¢"""
    print(f"\nğŸŸ¢ BM25å…¨æ–‡æ£€ç´¢:")
    
    try:
        # åˆ›å»ºBM25æ£€ç´¢å™¨
        bm25 = BM25Retriever.from_documents(documents)
        bm25.k = top_k
        
        # æ£€ç´¢
        results = bm25.get_relevant_documents(query)
        
        for i, doc in enumerate(results, 1):
            print(f"   {i}. {doc.page_content[:80]}...")
        
        return results
    except Exception as e:
        print(f"   âš ï¸  BM25æ£€ç´¢å¤±è´¥: {e}")
        return []

async def test():
    print("=" * 70)
    print("LangChainæ£€ç´¢æµ‹è¯•")
    print("=" * 70)
    
    # 1. åŠ è½½æ–‡æ¡£åˆ°LangChainæœåŠ¡
    documents = load_documents_from_chroma("test3")
    
    if len(documents) < 5:
        print("âŒ æ–‡æ¡£å¤ªå°‘ï¼Œæ— æ³•æµ‹è¯•")
        return
    
    # å–å‰100ä¸ªæ–‡æ¡£åŠ å¿«æµ‹è¯•é€Ÿåº¦
    test_docs = documents[:100]
    print(f"ğŸ“ ä½¿ç”¨å‰ {len(test_docs)} ä¸ªæ–‡æ¡£è¿›è¡Œæµ‹è¯•")
    
    # åˆå§‹åŒ–LangChainæœåŠ¡
    await langchain_rag_service.load_collection("test3", test_docs, use_faiss=False)
    
    # 2. æµ‹è¯•æŸ¥è¯¢
    from open_webui.services.langchain_rag_service import LangChainRAGQuery
    
    test_queries = [
        "é‡å¤å®šä½ç²¾åº¦",
        "CANopenæŠ¥æ–‡",
        "å…³èŠ‚å‹å·"
    ]
    
    for query in test_queries:
        print("\n" + "=" * 70)
        print(f"ğŸ“ æŸ¥è¯¢: {query}")
        print("=" * 70)
        
        # ä½¿ç”¨LangChainå®Œæ•´æŸ¥è¯¢
        rag_query = LangChainRAGQuery(
            query=query,
            collection_name="test3",
            top_k=3,
            mode="hybrid"
        )
        
        result = await langchain_rag_service.query(rag_query)
        
        print(f"\nâœ… æ–¹æ³•: {result.method}")
        print(f"ğŸ“„ æ‰¾åˆ° {len(result.documents)} ä¸ªæ–‡æ¡£")
        print(f"ğŸ“ å›ç­”: {result.answer[:100]}...")
        
        for i, doc in enumerate(result.documents, 1):
            print(f"\n  æ–‡æ¡£ {i}: {doc.page_content[:80]}...")
    
    print("\n" + "=" * 70)
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("=" * 70)

if __name__ == "__main__":
    asyncio.run(test())
